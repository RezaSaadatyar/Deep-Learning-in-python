{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome to Deep Learning with Keras and TensorFlow in Python**\n",
    "\n",
    "**Presented by: Reza Saadatyar (2024-2025)**<br/>\n",
    "**E-mail: Reza.Saadatyar@outlook.com**<br/>\n",
    "**[GitHub](https://github.com/RezaSaadatyar/Deep-Learning-in-python)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**<br/>\n",
    "▪ [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)<br/>\n",
    "▪ [Data Shuffling](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)<br/>\n",
    "▪ [Repeat Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat)<br/>\n",
    "▪ [Batching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)<br/>\n",
    "▪ Prepare TensorFlow datasets for training, validation, and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract, Transform, Load (ETL) pipeline:**<br/>\n",
    "▪ `Extract:` Data is gathered from various sources Cloud (e.g., Google Cloud Storage, AWS S3, or Azure Blob Storage), Databases (e.g., MySQL, PostgreSQL), and Local File System (this might include CSV files, JSON files, or other raw data stored locally).<br/>\n",
    "▪ `Transform:` Data is processed, cleaned, or reformatted to make it suitable for analysis or model training. Common transformations include: normalizing numerical data (e.g., scaling values between 0 and 1), encoding categorical data (e.g., one-hot encoding), handling missing values, and resizing images or tokenizing text (if working with image or NLP datasets).<br/>\n",
    "▪ `Load:` The transformed data is loaded into a target system, such as a device or storage for further use.<br/>\n",
    "\n",
    "`tf.data` a TensorFlow API, streamlines loading, preprocessing, and feeding data into models. It excels with large datasets, supporting streaming and parallel processing for efficiency. \n",
    "\n",
    "**Key tf.data methods for extraction:**<br/>\n",
    "▪ `tf.data.Dataset.from_tensor_slices():` Create a dataset from in-memory tensors (e.g., NumPy arrays).<br/>\n",
    "▪ `tf.data.TextLineDataset:` Load text files line by line (e.g., for CSVs or raw text).<br/>\n",
    "▪ `tf.data.TFRecordDataset:` Load data stored in TFRecord format, which is optimized for TensorFlow.<br/>\n",
    "▪ `tf.keras.utils.image_dataset_from_directory(): `Load image datasets directly from a directory structure (useful for image classification tasks).<br/>\n",
    "\n",
    "**Key tf.data methods for transformation:**<br/>\n",
    "▪ `dataset.map():` Apply a transformation function to each element.<br/>\n",
    "▪ `dataset.filter():` Filter out elements based on a condition.<br/>\n",
    "▪ `dataset.shuffle():` Randomize the dataset.<br/>\n",
    "▪ `dataset.batch():` Group elements into batches.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF000e' size=\"4.5\" face=\"Arial\"><b>Import modules</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple, Union\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#e4e706 size=\"4.8\" face=\"Arial\"><b>1️⃣ Dataset</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>,\n",
       " array([ 8,  3, 20, -1,  0,  1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a NumPy array with the given values\n",
    "x = np.array([8, 3, 20, -1, 0, 1])\n",
    "\n",
    "# Create a TensorFlow Dataset from the NumPy array using tf.data.Dataset.from_tensor_slices\n",
    "# This creates a dataset where each element is a slice of the input array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# The dataset is now ready for iteration or further processing\n",
    "dataset, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=8>\n",
      "1 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=3>\n",
      "2 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=20>\n",
      "3 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=-1>\n",
      "4 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=0>\n",
      "5 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=1>\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataset and print each element along with its index\n",
    "for ind, tensor in enumerate(dataset):\n",
    "    print(f\"{ind} → {tensor = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.int64, name=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the element specification of the dataset\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(5,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2D tensor with random uniform values (shape [100, 5])\n",
    "x = tf.random.uniform([100, 5])\n",
    "\n",
    "# Create a 1D tensor with random uniform integer values (shape [100]) ranging from 0 to 1\n",
    "y = tf.random.uniform([100], maxval=2, dtype=tf.int32)\n",
    "\n",
    "# Create a TensorFlow Dataset from a tuple of tensors (x, y) using tf.data.Dataset.from_tensor_slices\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "# Inspect the element specification of the dataset\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(5,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TensorFlow Dataset from the 2D tensor `x` using tf.data.Dataset.from_tensor_slices\n",
    "x_dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# Create a TensorFlow Dataset from the 1D tensor `y` using tf.data.Dataset.from_tensor_slices\n",
    "y_dataset = tf.data.Dataset.from_tensor_slices(y)\n",
    "\n",
    "# Combine the two datasets into a single dataset using tf.data.Dataset.zip\n",
    "# This pairs each element of `x_dataset` with the corresponding element of `y_dataset`\n",
    "dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\n",
    "\n",
    "# Inspect the element specification of the original dataset\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 → [0.69098985 0.80467045 0.7604947  0.0914799  0.6327827 ]\n",
      "1 → [0.99304974 0.5970018  0.21458507 0.7159656  0.7758702 ]\n",
      "1 → [0.31689167 0.5630431  0.2784543  0.00234151 0.65439403]\n",
      "1 → [0.50773513 0.10693932 0.40303254 0.27550995 0.6557487 ]\n",
      "0 → [0.4887359  0.44025254 0.05140471 0.75439227 0.35550952]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the first 5 elements of the dataset and print each pair of (x, y) values\n",
    "for ind_x, ind_y in dataset.take(5):\n",
    "    print(f\"{ind_y} → {ind_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind_y = <tf.Tensor: shape=(), dtype=int32, numpy=0> → ind_x = <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.69098985, 0.80467045, 0.7604947 , 0.0914799 , 0.6327827 ],\n",
      "      dtype=float32)>\n",
      "ind_y = <tf.Tensor: shape=(), dtype=int32, numpy=1> → ind_x = <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.99304974, 0.5970018 , 0.21458507, 0.7159656 , 0.7758702 ],\n",
      "      dtype=float32)>\n",
      "ind_y = <tf.Tensor: shape=(), dtype=int32, numpy=1> → ind_x = <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.31689167, 0.5630431 , 0.2784543 , 0.00234151, 0.65439403],\n",
      "      dtype=float32)>\n",
      "ind_y = <tf.Tensor: shape=(), dtype=int32, numpy=1> → ind_x = <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.50773513, 0.10693932, 0.40303254, 0.27550995, 0.6557487 ],\n",
      "      dtype=float32)>\n",
      "ind_y = <tf.Tensor: shape=(), dtype=int32, numpy=0> → ind_x = <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.4887359 , 0.44025254, 0.05140471, 0.75439227, 0.35550952],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "for ind_x, ind_y in dataset.take(5):\n",
    "    print(f\"{ind_y = } → {ind_x = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= #5ff309 size=\"4.8\" face=\"Arial\"><b>2️⃣ Data Shuffling</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 → [0.50773513 0.10693932 0.40303254 0.27550995 0.6557487 ]\n",
      "1 → [0.39901233 0.60831    0.1106385  0.68864775 0.3791287 ]\n",
      "1 → [0.31689167 0.5630431  0.2784543  0.00234151 0.65439403]\n",
      "1 → [0.6434109  0.12706244 0.13220489 0.9911444  0.3176396 ]\n",
      "1 → [0.99304974 0.5970018  0.21458507 0.7159656  0.7758702 ]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataset with a buffer size of 5\n",
    "# The `shuffle` method randomly shuffles the elements of the dataset using a buffer\n",
    "dataset = dataset.shuffle(buffer_size=5)\n",
    "\n",
    "# Iterate over the first 5 elements of the shuffled dataset and print each pair of (x, y) values\n",
    "for ind_x, ind_y in dataset.take(5):\n",
    "    print(f\"{ind_y} → {ind_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0ec3f0 size=\"4.8\" face=\"Arial\"><b>3️⃣ Repeat Dataset</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create a 1D tensor with values [0, 1, 2] using tf.range\n",
    "x = tf.range(3)\n",
    "\n",
    "# Create a TensorFlow Dataset from the tensor using tf.data.Dataset.from_tensor_slices\n",
    "x_dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# Repeat the dataset 2 times using the `repeat` method\n",
    "# This creates a dataset that iterates through the original dataset twice\n",
    "ds = x_dataset.repeat(2)\n",
    "\n",
    "# Iterate over the first 10 elements of the repeated dataset and print each element\n",
    "for ind_x in ds.take(10):\n",
    "    print(ind_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#e706af size=\"4.8\" face=\"Arial\"><b>4️⃣ Batching</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow Dataset with values from 0 to 99 using tf.data.Dataset.range\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "\n",
    "# Iterate over the first 5 elements of the dataset and print each element\n",
    "for ind in dataset.take(5):\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n",
      "tf.Tensor([12 13 14 15], shape=(4,), dtype=int64)\n",
      "tf.Tensor([16 17 18 19], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Batch the dataset into groups of 4 elements using the `batch` method\n",
    "ds = dataset.batch(4)\n",
    "\n",
    "# Iterate over the first 5 batches of the dataset and print each batch\n",
    "for ind in ds.take(5):\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#5b0cee size=\"4.5\" face=\"Arial\"><b>5️⃣ Prepare TensorFlow datasets for training, validation, and testing</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    data: Union[np.ndarray, tf.Tensor], \n",
    "    labels: Union[np.ndarray, tf.Tensor], \n",
    "    train_size: float = 0.8, \n",
    "    valid_size: float = 0.16, \n",
    "    batch_size: int = 16, \n",
    "    shuffle_train: bool = True, \n",
    "    shuffle_buffer_size: int = 1000\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    This function handles the complete pipeline from raw data to optimized TensorFlow Dataset objects,\n",
    "    including data splitting, shuffling, batching, and prefetching for optimal performance.\n",
    "    \n",
    "    Args:\n",
    "        data: Input features as either numpy array or TensorFlow tensor\n",
    "               Shape should be (num_samples, ...features_dims)\n",
    "        labels: Corresponding labels as either numpy array or TensorFlow tensor\n",
    "                Shape should be (num_samples, ...label_dims)\n",
    "        train_size: Proportion of total data to use for training (0.0 to 1.0)\n",
    "        valid_size: Proportion of total data to use for validation (0.0 to 1.0)\n",
    "        batch_size: Number of samples per training batch (positive integer)\n",
    "        shuffle_train: Whether to shuffle training data (recommended for training)\n",
    "        shuffle_buffer_size: Size of buffer used for shuffling (larger = better shuffling but more memory)\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing three tf.data.Dataset objects in order:\n",
    "        - train_dataset: Dataset for model training\n",
    "        - valid_dataset: Dataset for validation during training\n",
    "        - test_dataset: Dataset for final evaluation\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If input sizes are invalid or data/labels have mismatched lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================ INPUT VALIDATION ============================================\n",
    "    # Validate the split proportions make sense\n",
    "    if train_size + valid_size > 1.0:\n",
    "        raise ValueError(\"train_size + valid_size must not exceed 1.0 (test_size would be negative)\")\n",
    "    if train_size < valid_size:\n",
    "        raise ValueError(\"Training set should typically be larger than validation set\")\n",
    "\n",
    "    # Convert TensorFlow tensors to numpy arrays for sklearn splitting\n",
    "    if tf.is_tensor(data):\n",
    "        data = data.numpy()\n",
    "    if tf.is_tensor(labels):\n",
    "        labels = labels.numpy()\n",
    "\n",
    "    # Verify data and labels have compatible shapes\n",
    "    if len(data) != len(labels):\n",
    "        raise ValueError(f\"Mismatched lengths: data has {len(data)} samples but labels has {len(labels)}\")\n",
    "\n",
    "    # ======================================== DATA SPLITTING ==================================================\n",
    "    # First split separates test set from training+validation\n",
    "    x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
    "        data,\n",
    "        labels,\n",
    "        train_size=train_size + valid_size,  # Combined size for train+val\n",
    "        test_size=1.0 - (train_size + valid_size),  # Remainder for test\n",
    "        random_state=24,  # Fixed seed for reproducibility\n",
    "        # stratify=labels if len(set(labels)) > 1 else None  # Optional stratification\n",
    "    )\n",
    "\n",
    "    # Second split divides train+val into separate sets\n",
    "    # Calculate relative proportion of validation within train+val subset\n",
    "    valid_proportion = valid_size / (train_size + valid_size)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        x_train_val,\n",
    "        y_train_val,\n",
    "        train_size=1.0 - valid_proportion,  # Relative train size\n",
    "        test_size=valid_proportion,          # Relative validation size\n",
    "        random_state=24,  # Same seed for consistency\n",
    "        # stratify=y_train_val if len(set(y_train_val)) > 1 else None\n",
    "    )\n",
    "\n",
    "    # ======================================= DATASET CREATION =================================================\n",
    "    # Create TensorFlow Dataset objects with proper type casting\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(x_train, tf.float32),  # Convert features to float32\n",
    "        tf.cast(y_train, tf.float32)    # Convert labels to float32\n",
    "    ))\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(x_valid, tf.float32), \n",
    "        tf.cast(y_valid, tf.float32))\n",
    "    )\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.cast(x_test, tf.float32), \n",
    "        tf.cast(y_test, tf.float32))\n",
    "    )\n",
    "    \n",
    "    # ====================================== DATASET OPTIMIZATION ==============================================\n",
    "    # Shuffle training data if enabled (recommended for better training)\n",
    "    if shuffle_train:\n",
    "        train_dataset = train_dataset.shuffle(\n",
    "            buffer_size=min(shuffle_buffer_size, len(x_train)),  # Don't exceed dataset size\n",
    "            reshuffle_each_iteration=True  # Important for proper epoch training\n",
    "        )\n",
    "    \n",
    "    # Batch all datasets for efficient processing\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    valid_dataset = valid_dataset.batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    \n",
    "    # Prefetch data to overlap preprocessing and model execution\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)  # Let TensorFlow optimize buffer size\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # ============================================ VERIFICATION OUTPUT =========================================\n",
    "    print(f\"Training set:   {x_train.shape} features, {y_train.shape} labels\")\n",
    "    print(f\"Validation set: {x_valid.shape} features, {y_valid.shape} labels\")\n",
    "    print(f\"Test set:       {x_test.shape} features, {y_test.shape} labels\")\n",
    "    print(f\"\\nBatch size:     {batch_size}\")\n",
    "    print(f\"Training shuffle: {'enabled' if shuffle_train else 'disabled'}\")\n",
    "    print(\"\\t\")\n",
    "    pprint.pprint(train_dataset.element_spec, width=80)\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:   (640, 10) features, (640, 1) labels\n",
      "Validation set: (160, 10) features, (160, 1) labels\n",
      "Test set:       (200, 10) features, (200, 1) labels\n",
      "\n",
      "Batch size:     20\n",
      "Training shuffle: enabled\n",
      "\t\n",
      "(TensorSpec(shape=(None, 10), dtype=tf.float32, name=None),\n",
      " TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Define dimensions for synthetic data\n",
    "num_samples = 1000    # Number of samples\n",
    "num_features = 10     # Number of features per sample (e.g., tabular data)\n",
    "num_classes = 2       # Binary classification (adjustable)\n",
    "\n",
    "# Generate synthetic feature data\n",
    "# Random values between 0 and 1 (simulating normalized features)\n",
    "data = np.random.random(size=(num_samples, num_features))\n",
    "\n",
    "# Generate synthetic labels\n",
    "# Random binary labels (0 or 1) for classification\n",
    "labels = np.random.randint(0, num_classes, size=(num_samples, 1))\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "# Example usage with your U-Net\n",
    "# Assuming data_resize and masks_resize are your input data and masks\n",
    "train_dataset, valid_dataset, test_dataset = prepare_dataset(\n",
    "    data,\n",
    "    labels,\n",
    "    train_size=0.64,  # 64% of total\n",
    "    valid_size=0.16,  # 16% of total (20% of train+valid)\n",
    "    batch_size=batch_size,\n",
    "    shuffle_train=True,\n",
    "    shuffle_buffer_size=1000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
