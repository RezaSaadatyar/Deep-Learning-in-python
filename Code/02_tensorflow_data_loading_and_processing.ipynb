{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome to Deep Learning with Keras and TensorFlow in Python**\n",
    "\n",
    "**Presented by: Reza Saadatyar (2024-2025)**<br/>\n",
    "**E-mail: Reza.Saadatyar@outlook.com**<br/>\n",
    "**[GitHub](https://github.com/RezaSaadatyar/Deep-Learning-in-python)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract, Transform, Load (ETL) pipeline:**<br/>\n",
    "▪ `Extract:` Data is gathered from various sources Cloud (e.g., Google Cloud Storage, AWS S3, or Azure Blob Storage), Databases (e.g., MySQL, PostgreSQL), and Local File System (this might include CSV files, JSON files, or other raw data stored locally).<br/>\n",
    "▪ `Transform:` Data is processed, cleaned, or reformatted to make it suitable for analysis or model training. Common transformations include: normalizing numerical data (e.g., scaling values between 0 and 1), encoding categorical data (e.g., one-hot encoding), handling missing values, and resizing images or tokenizing text (if working with image or NLP datasets).<br/>\n",
    "▪ `Load:` The transformed data is loaded into a target system, such as a device or storage for further use.<br/>\n",
    "\n",
    "`tf.data` a TensorFlow API, streamlines loading, preprocessing, and feeding data into models. It excels with large datasets, supporting streaming and parallel processing for efficiency. \n",
    "\n",
    "**Key tf.data methods for extraction:**<br/>\n",
    "▪ `tf.data.Dataset.from_tensor_slices():` Create a dataset from in-memory tensors (e.g., NumPy arrays).<br/>\n",
    "▪ `tf.data.TextLineDataset:` Load text files line by line (e.g., for CSVs or raw text).<br/>\n",
    "▪ `tf.data.TFRecordDataset:` Load data stored in TFRecord format, which is optimized for TensorFlow.<br/>\n",
    "▪ `tf.keras.utils.image_dataset_from_directory(): `Load image datasets directly from a directory structure (useful for image classification tasks).<br/>\n",
    "\n",
    "**Key tf.data methods for transformation:**<br/>\n",
    "▪ `dataset.map():` Apply a transformation function to each element.<br/>\n",
    "▪ `dataset.filter():` Filter out elements based on a condition.<br/>\n",
    "▪ `dataset.shuffle():` Randomize the dataset.<br/>\n",
    "▪ `dataset.batch():` Group elements into batches.<br/>\n",
    "\n",
    "\n",
    "▪ <br/>\n",
    "▪ <br/>\n",
    "▪ <br/>\n",
    "▪ <br/>\n",
    "▪ <br/>\n",
    "▪ <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF000e' size=\"4.5\" face=\"Arial\"><b>Import modules</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>,\n",
       " array([ 8,  3, 20, -1,  0,  1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a NumPy array with the given values\n",
    "x = np.array([8, 3, 20, -1, 0, 1])\n",
    "\n",
    "# Create a TensorFlow Dataset from the NumPy array using tf.data.Dataset.from_tensor_slices\n",
    "# This creates a dataset where each element is a slice of the input array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# The dataset is now ready for iteration or further processing\n",
    "dataset, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=8>\n",
      "1 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=3>\n",
      "2 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=20>\n",
      "3 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=-1>\n",
      "4 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=0>\n",
      "5 → tensor = <tf.Tensor: shape=(), dtype=int64, numpy=1>\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataset and print each element along with its index\n",
    "for ind, tensor in enumerate(dataset):\n",
    "    print(f\"{ind} → {tensor = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.int64, name=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the element specification of the dataset\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(5,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2D tensor with random uniform values (shape [100, 5])\n",
    "x = tf.random.uniform([100, 5])\n",
    "\n",
    "# Create a TensorFlow Dataset from the 2D tensor using tf.data.Dataset.from_tensor_slices\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "# Create a 1D tensor with random uniform integer values (shape [100]) ranging from 0 to 1\n",
    "y = tf.random.uniform([100], maxval=2, dtype=tf.int32)\n",
    "\n",
    "# Create a TensorFlow Dataset from a tuple of tensors (x, y) using tf.data.Dataset.from_tensor_slices\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "# Inspect the element specification of the dataset\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid input to `zip`. Inputs are expected to be (nested) structures of `tf.data.Dataset` objects but encountered object of type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rsaad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1088\u001b[0m, in \u001b[0;36mDatasetV2.zip\u001b[1;34m(datasets, name, *args)\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1087\u001b[0m   datasets \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzip_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rsaad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\zip_op.py:24\u001b[0m, in \u001b[0;36m_zip\u001b[1;34m(datasets, name)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_zip\u001b[39m(datasets, name):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ZipDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rsaad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\zip_op.py:41\u001b[0m, in \u001b[0;36m_ZipDataset.__init__\u001b[1;34m(self, datasets, name)\u001b[0m\n\u001b[0;32m     35\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     36\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input to `zip`. Inputs are expected to be (nested)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m structures of `tf.data.Dataset` objects. Python `list` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not supported and you should use `tuple` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m       )\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     42\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input to `zip`. Inputs are expected to be (nested)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m structures of `tf.data.Dataset` objects but\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encountered object of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m       )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets \u001b[38;5;241m=\u001b[39m datasets\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets, [ds\u001b[38;5;241m.\u001b[39melement_spec \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets)]\n\u001b[0;32m     49\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid input to `zip`. Inputs are expected to be (nested) structures of `tf.data.Dataset` objects but encountered object of type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.zip((tf_dataset, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
