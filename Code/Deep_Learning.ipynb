{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artilces:<br/>\n",
    "[Perceptrons](https://direct.mit.edu/books/edited-volume/5431/chapter-abstract/3958520/1969-Marvin-Minsky-and-Seymour-Papert-Perceptrons?redirectedFrom=PDF)<br/>\n",
    "[The Organization of Behavior](https://pubmed.ncbi.nlm.nih.gov/10643472/)<br/>\n",
    "[Learning Internal Representations by Error Propagation](https://www.semanticscholar.org/paper/Learning-internal-representations-by-error-Rumelhart-Hinton/319f22bd5abfd67ac15988aa5c7f705f018c3ccd)<br/>\n",
    "[A logical calculus of the ideas immanent in nervous activity](https://link.springer.com/article/10.1007/BF02478259)<br/>\n",
    "[The perceptron: A probabilistic model for information storage and organization in the brain](https://www.semanticscholar.org/paper/The-perceptron%3A-a-probabilistic-model-for-storage-Rosenblatt/5d11aad09f65431b5d3cb1d85328743c9e53ba96)<br/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation algorithm**<br/>\n",
    "- Forward Pass\n",
    "  - `Input Layer:` The input features are fed into the network.\n",
    "  - `Hidden Layers:` Each neuron in a hidden layer sums up the weighted input from the previous layer and applies an activation function to produce its own output. This process continues through all hidden layers.\n",
    "  - `Output Layer:` The final layer produces the networkâ€™s output using the same process of weighted sums and activation.\n",
    "- Loss Calculation<br/>\n",
    "  - After the forward pass, compare the output of the network to the actual target values using a loss function (like mean squared error for regression tasks or cross-entropy for classification tasks).\n",
    "  - Calculate the total error (loss).\n",
    "- Backward Pass (Backpropagation)\n",
    "  - `Compute Output Error:` Determine the error at the output layer (the difference between the predicted and actual values).\n",
    "  - `Gradient of the Loss Function:` Calculate the gradient of the loss function with respect to the output of the network. This gradient will tell how much the loss would change with a small change in output.\n",
    "  - `Backpropagate the Error:`\n",
    "     - `Output to Hidden Layer:` For each neuron in the output layer, distribute its error backward to all neurons in the hidden layers that contribute directly to it, based on the strength (weight) of their connection and the gradient of the activation function used at the neurons.\n",
    "     - `Hidden Layers to Input:` Repeat this process for each hidden layer, moving from the outermost hidden layer to the input layer.\n",
    "- Update Weights and Biases\n",
    "  - `Calculate Gradient:` For each weight and bias, calculate the gradient of the loss function with respect to that parameter.\n",
    "  - `Adjust Parameters:` Update the weights and biases in the opposite direction of the gradient to minimize the loss. This is usually done using an optimizer like gradient descent. The size of the step taken in each update is determined by the learning rate.\n",
    "\n",
    "Repeat steps 1 through 4 for multiple epochs or until the network's performance stops improving. Each full pass through the training data is called an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
